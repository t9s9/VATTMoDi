{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import deepl\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = '/media/ts/SSD_ubuntu/datasets/AudioCaptionCarHospital/Car/Car_Label/car_zh_eval.json'\n",
    "df = pd.read_json(path)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_characters = df.caption.apply(lambda x: len(x)).sum()\n",
    "number_characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Translate with DEEPL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# API key in dotenv file\n",
    "translator = deepl.Translator(os.getenv('DEEPL'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eng_translation = [None for _ in range(df.shape[0])]\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    result = translator.translate_text(text=row.caption,\n",
    "                                       split_sentences=deepl.SplitSentences.NO_NEWLINES,\n",
    "                                       source_lang=deepl.Language.CHINESE,\n",
    "                                       target_lang=deepl.Language.ENGLISH_AMERICAN)\n",
    "    eng_translation[i] = result.text\n",
    "    time.sleep(0.1)\n",
    "df['caption_eng'] = eng_translation\n",
    "df.to_csv('/media/ts/SSD_ubuntu/datasets/AudioCaptionCarHospital/Car/Car_Label/car_eng_eval.json', index=False)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Translate with Google Translate (Hacky and Slow due to rate limit of Deepl)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome('/home/ts/Downloads/chromedriver_linux64/chromedriver', options=chrome_options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_translation(text: str):\n",
    "    try:\n",
    "        link = f\"https://translate.google.com/?hl=de&sl=zh-CN&tl=en&text={text}&op=translate\"\n",
    "        driver.get(link)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # accept cookies\n",
    "        cookie_accept = driver.find_elements(by=By.XPATH, value='//span[contains(text(), \"Alle akzeptieren\")]')\n",
    "        if cookie_accept:\n",
    "            cookie_accept[0].click()\n",
    "            time.sleep(2)\n",
    "\n",
    "        return True, driver.find_element(by=By.CLASS_NAME, value=\"Q4iAWc\").text\n",
    "    except Exception as e:\n",
    "        print(\"Failed with\", e)\n",
    "        return False, str(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = [None for _ in range(df.shape[0])]\n",
    "fail = {}\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    success, translation = get_translation(row.caption)\n",
    "    if success:\n",
    "        result[i] = translation\n",
    "    else:\n",
    "        fail[i] = translation\n",
    "df['caption_eng'] = result\n",
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Postprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hospital"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hroot = Path('/media/ts/SSD_ubuntu/datasets/AudioCapsHospital/')\n",
    "hdf_dev = pd.read_csv(hroot / 'labels/hospital_eng_dev.csv')\n",
    "hdf_eval = pd.read_csv(hroot / 'labels/hospital_eng_eval.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rename files to match the labels\n",
    "for file in Path(hroot / 'data').iterdir():\n",
    "    new_file = file.with_name(f'{int(file.stem):05d}').with_suffix(file.suffix)\n",
    "    file.rename(new_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df = df.drop(columns=['duration', 'caption', 'tokens', 'caption_index'])\n",
    "    df = df.groupby(by='filename')['caption_eng'].apply(list).reset_index()\n",
    "    df = df.rename(columns={'caption_eng': 'captions'})\n",
    "    df.filename = df.filename.apply(lambda f: Path(f).stem + '.mp4')\n",
    "    return df\n",
    "\n",
    "hdf_dev = process_df(hdf_dev)\n",
    "hdf_eval = process_df(hdf_eval)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir = hroot / 'train'\n",
    "dir.mkdir()\n",
    "not_exists = []\n",
    "for i, row in hdf_dev.iterrows():\n",
    "    path = hroot / 'data' / row['filename']\n",
    "    if path.exists():\n",
    "        path.rename(dir / row['filename'])\n",
    "    else:\n",
    "        not_exists.append(i)\n",
    "hdf_dev = hdf_dev.drop(index=not_exists, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (hroot / 'data').rename(hroot / 'val')\n",
    "not_exists = []\n",
    "for i, row in hdf_eval.iterrows():\n",
    "    path = hroot / 'val' / row['filename']\n",
    "    if not path.exists():\n",
    "        not_exists.append(i)\n",
    "hdf_eval = hdf_eval.drop(index=not_exists, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in (hroot / 'val').iterdir():\n",
    "    if file.name not in list(hdf_eval.filename):\n",
    "        file.unlink()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdf_dev.filename = hdf_dev.filename.apply(lambda f: str(Path('train') / f))\n",
    "hdf_eval.filename = hdf_eval.filename.apply(lambda f: str(Path('val') / f))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hdf_dev.to_parquet(hroot / 'annot_train.parquet', index=False)\n",
    "hdf_eval.to_parquet(hroot / 'annot_val.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Car"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "croot = Path('/media/ts/SSD_ubuntu/datasets/AudioCapsCar/')\n",
    "cdf_dev = process_df(pd.read_csv(croot / 'labels' / 'car_eng_dev.csv'))\n",
    "cdf_eval = process_df(pd.read_csv(croot / 'labels' / 'car_eng_eval.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_dir = croot / 'train'\n",
    "train_data_dir.mkdir()\n",
    "val_data_dir = croot / 'val'\n",
    "val_data_dir.mkdir()\n",
    "\n",
    "not_exists = []\n",
    "for i, row in cdf_dev.iterrows():\n",
    "    path = croot / 'data' / row['filename']\n",
    "    if path.exists():\n",
    "        path.rename(train_data_dir / row['filename'])\n",
    "    else:\n",
    "        not_exists.append(i)\n",
    "cdf_dev = cdf_dev.drop(index=not_exists, axis=0)\n",
    "\n",
    "not_exists = []\n",
    "for i, row in cdf_eval.iterrows():\n",
    "    path = croot / 'data' / row['filename']\n",
    "    if path.exists():\n",
    "        path.rename(val_data_dir / row['filename'])\n",
    "    else:\n",
    "        not_exists.append(i)\n",
    "cdf_eval = cdf_eval.drop(index=not_exists, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cdf_dev.filename = cdf_dev.filename.apply(lambda f: str(Path('train') / f))\n",
    "cdf_eval.filename = cdf_eval.filename.apply(lambda f: str(Path('val') / f))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cdf_dev.to_parquet(croot / 'annot_train.parquet', index=False)\n",
    "cdf_eval.to_parquet(croot / 'annot_val.parquet', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
