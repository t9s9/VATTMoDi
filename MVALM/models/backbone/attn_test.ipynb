{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import importlib\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "is_xformers_available = importlib.util.find_spec(\"xformers\") is not None\n",
    "if is_xformers_available:\n",
    "    import xformers\n",
    "    import xformers.ops\n",
    "else:\n",
    "    xformers = None\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MemoryEfficientAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_heads: int = 8,\n",
    "            qkv_bias: bool = False,\n",
    "            attn_drop: float = 0.0,\n",
    "            proj_drop: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % num_heads == 0), 'dim should be divisible by num_heads'\n",
    "        assert num_heads > 0\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_heads = dim // num_heads\n",
    "        self.attn_drop = attn_drop\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=False)\n",
    "\n",
    "        self._use_memory_efficient_attention_xformers = False\n",
    "        self.set_use_memory_efficient_attention_xformers(True)\n",
    "\n",
    "    def set_use_memory_efficient_attention_xformers(self, use_memory_efficient_attention_xformers: bool):\n",
    "        if use_memory_efficient_attention_xformers:\n",
    "            if not is_xformers_available:\n",
    "                raise ModuleNotFoundError(\n",
    "                    \"Refer to https://github.com/facebookresearch/xformers for more information on how to install\"\n",
    "                    \" xformers\",\n",
    "                    name=\"xformers\",\n",
    "                )\n",
    "            elif not torch.cuda.is_available():\n",
    "                raise ValueError(\n",
    "                    \"torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is\"\n",
    "                    \" only available for GPU \"\n",
    "                )\n",
    "            else:\n",
    "                try:\n",
    "                    # Make sure we can run the memory efficient attention\n",
    "                    _ = xformers.ops.memory_efficient_attention(\n",
    "                        torch.randn((1, 2, 40), device=\"cuda\"),\n",
    "                        torch.randn((1, 2, 40), device=\"cuda\"),\n",
    "                        torch.randn((1, 2, 40), device=\"cuda\"),\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "        self._use_memory_efficient_attention_xformers = use_memory_efficient_attention_xformers\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        # B : batch size\n",
    "        # S : sequence length\n",
    "        # D : embedding size\n",
    "        # H : number of heads\n",
    "        # K : embeddings size per head\n",
    "\n",
    "        B, N, C = x.size()\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, self.dim_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "            .flatten(1, 2)\n",
    "        )\n",
    "\n",
    "        q, k, v = qkv.unbind()\n",
    "\n",
    "        if self._use_memory_efficient_attention_xformers:\n",
    "            x = xformers.ops.memory_efficient_attention(\n",
    "                q, k, v, p=self.attn_drop, attn_bias=att_mask\n",
    "            )\n",
    "        else:\n",
    "            # todo implement using torch.baddbmm\n",
    "            q = q / math.sqrt(k.size(-1))\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            if att_mask is not None:\n",
    "                attn = attn + att_mask\n",
    "            attn = attn.softmax(-1)\n",
    "            attn = F.dropout(attn, self.attn_drop)\n",
    "            x =  attn @ v\n",
    "\n",
    "        x = (\n",
    "            x\n",
    "            .view(B, self.num_heads, N, self.dim_heads)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(B, N, C)\n",
    "        )\n",
    "\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class MemoryEfficientAttentionNew(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            num_heads: int = 8,\n",
    "            qkv_bias: bool = False,\n",
    "            attn_drop: float = 0.0,\n",
    "            proj_drop: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % num_heads == 0), 'dim should be divisible by num_heads'\n",
    "        assert num_heads > 0\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_heads = dim // num_heads\n",
    "        self.attn_drop = attn_drop\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=False)\n",
    "\n",
    "        self._use_memory_efficient_attention_xformers = False\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        # B : batch size\n",
    "        # S : sequence length\n",
    "        # D : embedding size\n",
    "        # H : number of heads\n",
    "        # K : embeddings size per head\n",
    "\n",
    "        B, S, D = x.size()                                     # B x S x D\n",
    "        qkv = (\n",
    "            self.qkv(x)                                        # B x S x D*3\n",
    "            .reshape(B, S, 3, self.num_heads, self.dim_heads)  # B x S x 3 x H x (D//H)\n",
    "            .permute(2, 0, 3, 1, 4)                            # 3 x B x H x S x (D//H)\n",
    "            .flatten(1, 2)                                     # 3 x (B*H) x S x (D//H)\n",
    "        )\n",
    "\n",
    "        q, k, v = qkv.unbind() # (B*H) x S x (D//H)\n",
    "\n",
    "        if self._use_memory_efficient_attention_xformers:\n",
    "            x = xformers.ops.memory_efficient_attention(\n",
    "                q, k, v, p=self.attn_drop, attn_bias=att_mask\n",
    "            )\n",
    "        else:\n",
    "            # todo implement using torch.baddbmm\n",
    "            q = q / math.sqrt(k.size(-1))\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            if att_mask is not None:\n",
    "                attn = attn + att_mask\n",
    "            attn = attn.softmax(-1)\n",
    "            attn = F.dropout(attn, self.attn_drop)\n",
    "            x =  attn @ v\n",
    "\n",
    "\n",
    "        x = (\n",
    "            x                                            # (B*H) x S x (D//H)\n",
    "            .view(B, self.num_heads, S, self.dim_heads)  # B x H x S x (D//H)\n",
    "            .transpose(1, 2)                             # B x S x H x (D//H)\n",
    "            .reshape(B, S, D)                            # B x S x D\n",
    "        )\n",
    "\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "attn = MemoryEfficientAttentionNew(dim=128, num_heads=8).to('cuda').train()\n",
    "\n",
    "x = torch.randn((2, 512, 128), device=\"cuda\")\n",
    "\n",
    "out_eff = attn(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class MemoryEfficientAttentionOne(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            attn_drop=0.0,\n",
    "            proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % num_heads == 0), 'dim should be divisible by num_heads'\n",
    "        assert num_heads > 0\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_heads = dim // num_heads\n",
    "        self.attn_drop = attn_drop\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=False)\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        B, N, C = x.size()\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, self.dim_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "            .flatten(1, 2)\n",
    "        )\n",
    "\n",
    "        q, k, v = qkv.unbind()\n",
    "\n",
    "        x = xformers.ops.memory_efficient_attention(\n",
    "            q, k, v, p=self.attn_drop, attn_bias=att_mask\n",
    "        )\n",
    "\n",
    "        x = (\n",
    "            x\n",
    "            .view(B, self.num_heads, N, self.dim_heads)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(B, N, C)\n",
    "        )\n",
    "\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class MemoryEfficientAttentionTwo(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias: bool = False,\n",
    "            attn_drop=0.0,\n",
    "            proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % num_heads == 0), 'dim should be divisible by num_heads'\n",
    "        assert num_heads > 0\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_heads = dim // num_heads\n",
    "        self.attn_drop = attn_drop\n",
    "\n",
    "        self.proj_query = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.proj_key = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.proj_value = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=False)\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: Optional[torch.Tensor] = None,\n",
    "                value: Optional[torch.Tensor] = None,\n",
    "                att_mask: Optional[torch.Tensor] = None, ):\n",
    "        \"\"\"\n",
    "        Expected input dimensions are [batch size, sequence length, embed dim]\n",
    "        Output dimensions are [batch size, sequence length, embed dim]\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        B, S_Q, _ = query.size()  # Batch x Sequence x Embedding (latent)\n",
    "        _, S_K, _ = key.size()  # K, Q's sequence length could differ\n",
    "\n",
    "        q = self.proj_query(query).view(B, S_Q, self.num_heads, self.dim_heads).transpose(1, 2)\n",
    "        k = self.proj_key(key).view(B, S_K, self.num_heads, self.dim_heads).transpose(1, 2)\n",
    "        v = self.proj_value(value).view(B, S_K, self.num_heads, self.dim_heads).transpose(1, 2)\n",
    "\n",
    "        # Input tensors must be in format [B, M, H, K], where B is the batch size, M\n",
    "        # the sequence length, H the number of heads, and K the embedding size per head\n",
    "        x = xformers.ops.memory_efficient_attention(\n",
    "            q, k, v, p=self.attn_drop, attn_bias=att_mask\n",
    "        )\n",
    "\n",
    "        x = (\n",
    "            x\n",
    "            .view(B, self.num_heads, S_Q, self.dim_heads)\n",
    "            .transpose(1, 2)\n",
    "            .flatten(start_dim=2, end_dim=3)\n",
    "        )\n",
    "\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Memory: 1406.0 Time: 8.993\n",
      "Max Memory: 1357.0 Time: 5.708809999999999\n"
     ]
    }
   ],
   "source": [
    "def benchmark_model(model, input_size, device, dtype, warmup=2, runs=10, kwargs={}):\n",
    "    model = model.to(device=device)\n",
    "    model.train()\n",
    "    mem, t = [], []\n",
    "    with torch.autocast(device_type='cuda', dtype=dtype):\n",
    "        x = torch.randn(input_size, device=device, requires_grad=True)\n",
    "\n",
    "        for i in range(runs + warmup):\n",
    "            start = time.time()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "            y = model(x)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            stop = time.time()\n",
    "\n",
    "            # now report\n",
    "            max_memory = torch.cuda.max_memory_allocated() // 2 ** 20\n",
    "            diff = round((stop - start) * 1e6) / 1e3\n",
    "\n",
    "            if i >= warmup:\n",
    "                mem.append(max_memory)\n",
    "                t.append(diff)\n",
    "    print(\"Max Memory:\", np.mean(max_memory), \"Time:\", np.mean(t))\n",
    "    return None\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "DTYPE = torch.float16\n",
    "BATCH = 64\n",
    "HEADS = 12\n",
    "SEQ = 512\n",
    "EMB = 768\n",
    "\n",
    "benchmark_model(MemoryEfficientAttentionTwo(dim=EMB, num_heads=HEADS),\n",
    "                input_size=(BATCH, SEQ, EMB),\n",
    "                dtype=DTYPE, device=DEVICE, runs=100, warmup=10)\n",
    "benchmark_model(MemoryEfficientAttentionOne(dim=EMB, num_heads=HEADS),\n",
    "                input_size=(BATCH, SEQ, EMB),\n",
    "                dtype=DTYPE, device=DEVICE, runs=100, warmup=10)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 dropout: float = 0.0,\n",
    "                 activation: nn.Module = nn.GELU,\n",
    "                 hidden_layer_multiplier: int = 4,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        dim_mlp = hidden_layer_multiplier * dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=dim, out_features=dim_mlp, bias=bias),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=dim_mlp, out_features=dim, bias=bias),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = MemoryEfficientAttentionTwo(dim=dim, num_heads=num_heads)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, num_layers: int,\n",
    "                 gradient_checkpointing: bool = False):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        print(\"Using gradient checkpointing: \", gradient_checkpointing)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[AttentionBlock(dim, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            return checkpoint_sequential(functions=self.blocks, input=x, segments=self.num_layers)\n",
    "        return self.blocks(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m SEQ \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m512\u001B[39m\n\u001B[1;32m      6\u001B[0m EMB \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1024\u001B[39m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmem_eff_transformer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Transformer\n\u001B[1;32m     10\u001B[0m attn \u001B[38;5;241m=\u001B[39m Transformer(dim\u001B[38;5;241m=\u001B[39mEMB, num_heads\u001B[38;5;241m=\u001B[39mHEADS, num_layers\u001B[38;5;241m=\u001B[39mLAYERS)\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     11\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(BATCH, SEQ, EMB, device\u001B[38;5;241m=\u001B[39mDEVICE, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/PycharmProjects/Multimodal-VAL-Models/MVALM/models/backbone/mem_eff_transformer.py:5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcheckpoint\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m checkpoint_sequential\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattention\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MemoryEfficientAttention\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_attn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossAttention\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mxformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfmha\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MemoryEfficientAttentionFlashAttentionOp\n",
      "\u001B[0;31mImportError\u001B[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda'\n",
    "BATCH = 8\n",
    "HEADS = 8\n",
    "LAYERS = 8\n",
    "SEQ = 512\n",
    "EMB = 1024\n",
    "\n",
    "\n",
    "attn = Transformer(dim=EMB, num_heads=HEADS, num_layers=LAYERS).to(DEVICE)\n",
    "x = torch.randn(BATCH, SEQ, EMB, device=DEVICE, requires_grad=True)\n",
    "x = attn(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
