{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autor\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "\n",
    "from MVALM.models.encoder import AudioEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t9s9/miniconda3/envs/fasttorch/lib/python3.10/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/t9s9/miniconda3/envs/fasttorch/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "htsat - Parameters: 27,534,488; Hidden Size: 768\n"
     ]
    }
   ],
   "source": [
    "model = AudioEncoder(model_name='htsat', gradient_checkpointing=False)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 160000])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = librosa.load('/home/t9s9/Datasets/AudioCaps/audio/val/rqfQRErjfk8_000170_000180.flac',\n",
    "                         mono=True, sr=32000, duration=5)\n",
    "audio_input = torch.from_numpy(audio).unsqueeze(0)\n",
    "audio_input.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 501, 64]) 4\n",
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast(enabled=False):\n",
    "        spec = model.spectrogram(audio_input)\n",
    "        print(spec.shape, spec.ndim)\n",
    "    print(model.model.reshape_wav2img(spec).shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.dim=96 self.window_size=(8, 8) self.num_heads=4\n",
      "torch.Size([64, 4, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([4, 64, 64])\n",
      "self.dim=96 self.window_size=(8, 8) self.num_heads=4\n",
      "torch.Size([64, 4, 64, 24])\n",
      "torch.Size([64, 64, 64])\n",
      "----------\n",
      "BIAS torch.Size([4, 64, 64])\n",
      "self.dim=192 self.window_size=(8, 8) self.num_heads=8\n",
      "torch.Size([16, 8, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([8, 64, 64])\n",
      "self.dim=192 self.window_size=(8, 8) self.num_heads=8\n",
      "torch.Size([16, 8, 64, 24])\n",
      "torch.Size([16, 64, 64])\n",
      "----------\n",
      "BIAS torch.Size([8, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "torch.Size([4, 64, 64])\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "torch.Size([4, 64, 64])\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=384 self.window_size=(8, 8) self.num_heads=16\n",
      "torch.Size([4, 16, 64, 24])\n",
      "torch.Size([4, 64, 64])\n",
      "----------\n",
      "BIAS torch.Size([16, 64, 64])\n",
      "self.dim=768 self.window_size=(8, 8) self.num_heads=32\n",
      "torch.Size([1, 32, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([32, 64, 64])\n",
      "self.dim=768 self.window_size=(8, 8) self.num_heads=32\n",
      "torch.Size([1, 32, 64, 24])\n",
      "None\n",
      "----------\n",
      "BIAS torch.Size([32, 64, 64])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(spec)\n",
    "    print(out.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test AudioSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MultilabelAveragePrecision, MultilabelPrecision, \\\n",
    "    MultilabelRecall, MultilabelExactMatch\n",
    "from tqdm import tqdm\n",
    "from MVALM.datasets.utils import multi_one_hot\n",
    "from MVALM.datasets.AudioSet.audioset import AudioSetAudioOnly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:21:39,460 - INFO - Initialized dataset AudioSet\n",
      "2023-03-21 12:21:39,487 - INFO - Loaded annotation with 17092 samples.\n",
      "2023-03-21 12:21:39,494 - INFO - Verifying files for AudioSet...\n",
      "2023-03-21 12:21:39,628 - WARNING - Found 6 missing files.\n",
      "2023-03-21 12:21:39,628 - WARNING - Removing missing files from the dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: AudioSet, Split: val, Size: 17086, Label: caption\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model_htsat()\n",
    "model.to(device).eval()\n",
    "\n",
    "dataset = AudioSetAudioOnly(split='val', datasets_root='/media/t9s9/SSD_ubuntu/datasets/',\n",
    "                            verbose=True, sample_rate=32000, length=32000 * 10, mono=True)\n",
    "dataset.verify_files()\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    audio = torch.stack([item.audio for item in batch])\n",
    "    target = torch.stack([torch.from_numpy(multi_one_hot(item.label, 527)) for item in batch])\n",
    "    return audio, target\n",
    "\n",
    "\n",
    "dl = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "metric_kwags = dict(num_labels=dataset.num_classes)\n",
    "metric = MetricCollection(\n",
    "    [MultilabelAveragePrecision(**metric_kwags),\n",
    "     MultilabelExactMatch(**metric_kwags),\n",
    "     MultilabelPrecision(**metric_kwags),\n",
    "     MultilabelRecall(**metric_kwags)]\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/267 [00:00<?, ?it/s]/home/t9s9/miniconda3/envs/fasttorch/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)\n",
      " 37%|███▋      | 100/267 [00:32<00:54,  3.07it/s, MultilabelAveragePrecision=tensor(0.0620, device='cuda:0'), MultilabelExactMatch=tensor(0., device='cuda:0'), MultilabelPrecision=tensor(0.0031, device='cuda:0'), MultilabelRecall=tensor(0.0876, device='cuda:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilabelAveragePrecision    : 0.007\n",
      "MultilabelExactMatch          : 0.000\n",
      "MultilabelPrecision           : 0.004\n",
      "MultilabelRecall              : 0.500\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(dl), total=len(dl))\n",
    "    for i, batch in bar:\n",
    "        audio, label = batch\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        logits = model(audio)  #['clipwise_output']\n",
    "\n",
    "        step_metric = metric(logits, label.clone())\n",
    "        bar.set_postfix(step_metric)\n",
    "\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "for k, v in metric.compute().items():\n",
    "    print(f\"{k:<30}: {v.detach().cpu().item():.3f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
